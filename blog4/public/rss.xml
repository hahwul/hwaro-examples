<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DevLog</title>
    <link>http://localhost:3000</link>
    <description>Today I Learned &amp; Changelog</description>
    <atom:link href="http://localhost:3000/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TIL: Crystal Fibers for Concurrency</title>
      <link>http://localhost:3000/log/til-crystal-fibers/</link>
      <guid>http://localhost:3000/log/til-crystal-fibers/</guid>
      <description>&lt;p&gt;Today I learned how Crystal handles concurrency with &lt;strong&gt;Fibers&lt;/strong&gt; — lightweight green threads managed by the runtime.&lt;/p&gt;
&lt;h2 id=&quot;the-basics&quot;&gt;The basics&lt;/h2&gt;
&lt;p&gt;Crystal uses &lt;code&gt;spawn&lt;/code&gt; to create a fiber and &lt;code&gt;Channel&lt;/code&gt; to communicate between them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;ch = Channel(Int32).new

spawn do
  result = expensive_computation()
  ch.send(result)
end

# Do other work while fiber runs
value = ch.receive
puts &amp;quot;Got: #{value}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fibers are &lt;strong&gt;cooperative&lt;/strong&gt;, not preemptive — they yield at IO boundaries&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Channel&lt;/code&gt; is typed and can be buffered or unbuffered&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;select&lt;/code&gt; for waiting on multiple channels&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Fibers are incredibly cheap to create. Spawning 100k fibers is totally fine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;gotchas&quot;&gt;Gotchas&lt;/h2&gt;
&lt;p&gt;Be careful with shared mutable state. Crystal doesn&apos;t have a borrow checker — it&apos;s up to you to avoid data races:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;# BAD: shared mutable array
results = [] of Int32
10.times do |i|
  spawn { results &amp;lt;&amp;lt; i }
end
sleep 0.1

# GOOD: use a channel instead
ch = Channel(Int32).new
10.times do |i|
  spawn { ch.send(i) }
end
10.times { results &amp;lt;&amp;lt; ch.receive }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{{&amp;lt; alert type=&amp;quot;tip&amp;quot; message=&amp;quot;Always prefer channels over shared state for inter-fiber communication.&amp;quot; &amp;gt;}}&lt;/p&gt;
</description>
      <pubDate>Wed, 11 Jun 2025 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Changelog: Project Atlas v2.0</title>
      <link>http://localhost:3000/log/changelog-v2/</link>
      <guid>http://localhost:3000/log/changelog-v2/</guid>
      <description>&lt;p&gt;{{&amp;lt; version number=&amp;quot;2.0.0&amp;quot; &amp;gt;}} — Released 2025-06-10&lt;/p&gt;
&lt;h2 id=&quot;whats-new&quot;&gt;What&apos;s new&lt;/h2&gt;
&lt;h3 id=&quot;parallel-build-pipeline&quot;&gt;Parallel build pipeline&lt;/h3&gt;
&lt;p&gt;The build system now processes files in parallel using a fiber pool. This reduces build times by &lt;strong&gt;60-70%&lt;/strong&gt; on multi-core machines.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;ParallelHelper.execute(files) do |file|
  process(file)
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;new-configuration-format&quot;&gt;New configuration format&lt;/h3&gt;
&lt;p&gt;We&apos;ve migrated from YAML to TOML for the config file. The new format is stricter but catches errors earlier.&lt;/p&gt;
&lt;h3 id=&quot;breaking-changes&quot;&gt;Breaking changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;config.yml&lt;/code&gt; is no longer supported — migrate to &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;--legacy&lt;/code&gt; flag has been removed&lt;/li&gt;
&lt;li&gt;Minimum Crystal version bumped to 1.12&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;migration-guide&quot;&gt;Migration guide&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Rename &lt;code&gt;config.yml&lt;/code&gt; to &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Convert YAML syntax to TOML (see &lt;a href=&quot;https://example.com&quot;&gt;converter tool&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;atlas check&lt;/code&gt; to validate your new config&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;{{&amp;lt; alert type=&amp;quot;warning&amp;quot; message=&amp;quot;Back up your &lt;code&gt;config.yml&lt;/code&gt; before migrating. The converter is one-way.&amp;quot; &amp;gt;}}&lt;/p&gt;
&lt;h2 id=&quot;bug-fixes&quot;&gt;Bug fixes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fixed memory leak in file watcher (#142)&lt;/li&gt;
&lt;li&gt;Corrected permalink generation for nested sections (#156)&lt;/li&gt;
&lt;li&gt;Resolved feed generation crash when posts have no date (#161)&lt;/li&gt;
&lt;/ul&gt;
</description>
      <pubDate>Mon, 9 Jun 2025 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Debugging Memory Leaks in Long-Running Processes</title>
      <link>http://localhost:3000/log/debugging-memory-leaks/</link>
      <guid>http://localhost:3000/log/debugging-memory-leaks/</guid>
      <description>&lt;p&gt;Last week we hit a memory leak in production — the process grew from 200MB to 4GB over 48 hours. Here&apos;s how we tracked it down.&lt;/p&gt;
&lt;h2 id=&quot;symptoms&quot;&gt;Symptoms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RSS memory climbed steadily with no plateau&lt;/li&gt;
&lt;li&gt;GC runs became more frequent but didn&apos;t reclaim memory&lt;/li&gt;
&lt;li&gt;No obvious large allocations in the code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;investigation&quot;&gt;Investigation&lt;/h2&gt;
&lt;h3 id=&quot;step-1-heap-snapshots&quot;&gt;Step 1: Heap snapshots&lt;/h3&gt;
&lt;p&gt;We added a debug endpoint to dump heap statistics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;get &amp;quot;/debug/heap&amp;quot; do
  stats = GC.stats
  {
    heap_size:  stats.heap_size,
    free_bytes: stats.free_bytes,
    total_bytes: stats.total_bytes,
  }.to_json
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;step-2-bisect-by-time&quot;&gt;Step 2: Bisect by time&lt;/h3&gt;
&lt;p&gt;By comparing snapshots, we found that &lt;code&gt;String&lt;/code&gt; allocations were growing unbounded. The culprit was a logging buffer that was never flushed:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;# BUG: buffer grows forever
class Logger
  @buffer = [] of String

  def log(msg : String)
    @buffer &amp;lt;&amp;lt; &amp;quot;[#{Time.utc}] #{msg}&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;step-3-the-fix&quot;&gt;Step 3: The fix&lt;/h3&gt;
&lt;p&gt;We added a ring buffer with a fixed capacity:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-crystal hljs&quot;&gt;class Logger
  CAPACITY = 10_000
  @buffer = Deque(String).new

  def log(msg : String)
    @buffer.shift if @buffer.size &amp;gt;= CAPACITY
    @buffer &amp;lt;&amp;lt; &amp;quot;[#{Time.utc}] #{msg}&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{{&amp;lt; alert type=&amp;quot;tip&amp;quot; message=&amp;quot;Always set upper bounds on in-memory buffers. Unbounded collections are a common source of leaks.&amp;quot; &amp;gt;}}&lt;/p&gt;
&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Monitor RSS, not just GC stats&lt;/strong&gt; — GC may report low usage while the OS sees high RSS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Add debug endpoints&lt;/strong&gt; — they cost almost nothing and save hours during incidents&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set limits on all buffers&lt;/strong&gt; — if it can grow, it will grow&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The best time to add memory monitoring is before you need it.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
      <pubDate>Sat, 7 Jun 2025 15:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
